% assignment_3.tex - Assignment 3 for Machine Learning class (Spring 2015)
% Chanmann Lim - March 2015

\documentclass[a4paper]{article}

\usepackage[margin=1 in]{geometry}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage{float}

\everymath{\displaystyle}

\begin{document}
\setcounter{page}{3}
% \thispagestyle{empty}
% \pagestyle{empty}

% Part I
\subsection*{3. Part I}

% a
\paragraph{a.} $\mu$ and $\Sigma$ from the first 10 data samples: \\
\begin{align*}
	\mu = \begin{bmatrix}
		0.8190 & -0.6271
	\end{bmatrix}^{T}, \quad
	\Sigma = \begin{bmatrix}
		 0.7461  &  -0.1474 \\
		-0.1474  &   1.6047
	\end{bmatrix}
\end{align*}

% b
\paragraph{b.} $\mu$ and $\Sigma$ from the first 100 data samples: \\
\begin{align*}
	\mu = \begin{bmatrix}
		0.9977 & -0.9725
	\end{bmatrix}^{T}, \quad
	\Sigma = \begin{bmatrix}
		2.2580  &  1.0856 \\
		1.0856  &  2.1439
	\end{bmatrix}
\end{align*}

% c
\paragraph{c.} $\mu$ and $\Sigma$ from the first 1000 data samples: \\
\begin{align*}
	\mu = \begin{bmatrix}
		1.0222 & -0.9629
	\end{bmatrix}^{T}, \quad
	\Sigma = \begin{bmatrix}
		2.2118  &  1.1878 \\
		1.1878  &  2.0332
	\end{bmatrix}
\end{align*}

% d
\paragraph{c.} $\mu$ and $\Sigma$ from the first 10000 data samples: \\
\begin{align*}
	\mu = \begin{bmatrix}
		0.9947 & -1.0027
	\end{bmatrix}^{T}, \quad
	\Sigma = \begin{bmatrix}
		1.9978  &  0.9643 \\
		0.9643  &  1.9237
	\end{bmatrix}
\end{align*}

% e
\paragraph{e.} Parameter estimation errors \\

Measure 1:
	\begin{tabular}{l *{4}{c}}
			case      &   a    &   b    &   c    &   d    \\ \hline
		$\varepsilon$ & 1.7935 & 0.3088 & 0.2883 & 0.0845
	\end{tabular} \\
\vspace{2em}
 
Measure 2:
	\begin{tabular}{l *{4}{c}}
				case            &   a    &   b    &   c    &   d    \\ \hline
		$\varepsilon _{\mu}$    & 0.2931 & 0.0195 & 0.0306 & 0.0042 \\ 
		$\varepsilon _{\Sigma}$ & 1.0075 & 0.1776 & 0.1646 & 0.0487 
	\end{tabular} \\
\vspace{2em}

In both \emph{Measure 1} and \emph{Measure 2} we notice that parameter estimation errors decrease as the number of data samples increase. Maximum likelihood estimation assumes that the parameter $\boldsymbol{\theta}$ is fixed then seeks to find the parameter value to maximize the probability of the training data being observed.

% f
\paragraph{e.} Plot of first 100 data samples and 2D contours of estimated Gaussian pdf \\
\begin{figure}[H]
  \centering
    \includegraphics[scale=.44]{images/3_f_I.png}
  \caption{100 data samples and estimated Gaussian pdf 2D contours}
\end{figure}

% Part II
\subsection*{Part II}

% a
\paragraph{a.} $\mu$ and $\Sigma$ from the first 10 data samples: \\
\begin{align*}
	\mu = \begin{bmatrix}
		1.8829 & -1.8135
	\end{bmatrix}^{T}, \quad
	\Sigma = \begin{bmatrix}
		 5.6385  & -5.3104 \\
		-5.3104  &  5.3521
	\end{bmatrix}
\end{align*}

% b
\paragraph{b.} $\mu$ and $\Sigma$ from the first 100 data samples: \\
\begin{align*}
	\mu = \begin{bmatrix}
		1.1741 & -1.2216
	\end{bmatrix}^{T}, \quad
	\Sigma = \begin{bmatrix}
		 2.6753  & -2.5961 \\
		-2.5961  &  2.6913
	\end{bmatrix}
\end{align*}

% c
\paragraph{c.} $\mu$ and $\Sigma$ from the first 1000 data samples: \\
\begin{align*}
	\mu = \begin{bmatrix}
		0.9539 & -0.9530
	\end{bmatrix}^{T}, \quad
	\Sigma = \begin{bmatrix}
		 1.9939  & -1.9344 \\
		-1.9344  &  2.0528
	\end{bmatrix}
\end{align*}

% d
\paragraph{c.} $\mu$ and $\Sigma$ from the first 10000 data samples: \\
\begin{align*}
	\mu = \begin{bmatrix}
		1.0023 & -1.0031
	\end{bmatrix}^{T}, \quad
	\Sigma = \begin{bmatrix}
		 1.9659  & -1.8639 \\
		-1.8639  &  1.9582
	\end{bmatrix}
\end{align*}

% e
\paragraph{e.} Parameter estimation errors \\

Measure 1:
	\begin{tabular}{l *{4}{c}}
			case      &   a    &   b    &   c    &   d    \\ \hline
		$\varepsilon$ & 6.1275 & 1.2239 & 0.0914 & 0.0651
	\end{tabular} \\
\vspace{2em}
 
Measure 2: 
	\begin{tabular}{l *{4}{c}}
				case            &   a    &   b    &   c    &   d    \\ \hline
		$\varepsilon _{\mu}$    & 0.8489 & 0.1993 & 0.0466 & 0.0028 \\ 
		$\varepsilon _{\Sigma}$ & 3.4692 & 0.6876 & 0.0366 & 0.0375 
	\end{tabular} \\
\vspace{2em}

In both \emph{Measure 1} and \emph{Measure 2} we notice that parameter estimation errors decrease as the number of data samples increase. Maximum likelihood estimation assumes that the parameter $\boldsymbol{\theta}$ is fixed then seeks to find the parameter value to maximize the probability of the training data being observed.

% f
\paragraph{e.} Plot of first 100 data samples and 2D contours of estimated Gaussian pdf \\
\begin{figure}[H]
  \centering
    \includegraphics[scale=.44]{images/3_f_II.png}
  \caption{100 data samples and estimated Gaussian pdf 2D contours}
\end{figure}

\newpage
\subsection*{Appendix:}
\lstinputlisting[language=Matlab, title=\lstname, basicstyle=\footnotesize]{assignment_3.m}
\lstinputlisting[language=Matlab, title=\lstname, basicstyle=\footnotesize]{problem_3_report.m}
\lstinputlisting[language=Matlab, title=\lstname, basicstyle=\footnotesize]{mle.m}
\lstinputlisting[language=Matlab, title=\lstname, basicstyle=\footnotesize]{theta.m}
\lstinputlisting[language=Matlab, title=\lstname, basicstyle=\footnotesize]{error_measure_1.m}
\lstinputlisting[language=Matlab, title=\lstname, basicstyle=\footnotesize]{error_measure_2.m}
































\end{document}